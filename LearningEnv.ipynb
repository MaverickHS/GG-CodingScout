{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6423a1",
   "metadata": {},
   "source": [
    "Start by deploying LLM `Mistral 7B Instruct` to Baseten by following the documentation [here](https://www.baseten.co/library/mistral-7b-instruct/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a391591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing Minimal Baseten Client\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Baseten configuration\n",
    "BASETEN_API_KEY = os.getenv('BASETEN_API_KEY')\n",
    "BASE_URL = os.getenv('BASE_URL')\n",
    "MODEL_NAME = os.getenv('MODEL_NAME')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=BASETEN_API_KEY,\n",
    "    base_url=BASE_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d2759",
   "metadata": {},
   "source": [
    "Begin interfacing with the deployed LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the chat completions endpoint\n",
    "response_chat = client.chat.completions.create(\n",
    "        model=MODEL_NAME, # type: ignore to suppress type checking error\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": \"Write a short poem about the sea.\"}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "\n",
    "assistant_message = response_chat.choices[0].message.content\n",
    "print(f\"\\nRESPONSE:\\n{assistant_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78486ccd",
   "metadata": {},
   "source": [
    "Moved on to zero-shot categorization model from HuggingFace:\n",
    "\n",
    "started by installing truss prerequsite:\n",
    "`pip install --upgrade truss 'pydantic>=2.0.0'`\n",
    "\n",
    "next, I initialized the model\n",
    "\n",
    "```bash\n",
    "$ truss init cat-model\n",
    "? ðŸ“¦ Name this model: categorize-model\n",
    "Truss categorize-model was created in ~/cat-model\n",
    "```\n",
    "then I setup my config.yaml by adding a descripting and requirements\n",
    "```yaml\n",
    "description: Zero-shot classifier for categorizing elder tech support issues\n",
    "requirements:\n",
    "  - torch\n",
    "  - transformers\n",
    "  - accelerate\n",
    "```\n",
    "next, I changed model.py to use the transformers library to load the model\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Password Issue\",\n",
    "    \"App Navigation\",\n",
    "    \"Device Setup\",\n",
    "    \"Email Problem\",\n",
    "    \"Security Concern\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.classifier = None\n",
    "\n",
    "    def load(self):\n",
    "        # Load zero-shot classifier\n",
    "        try:\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=\"MoritzLaurer/deberta-v3-base-zeroshot-v2.0\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.classifier = None\n",
    "            print(f\"Failed to load classifier pipeline: {e}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        # Classify\n",
    "        if self.classifier is None:\n",
    "            raise RuntimeError(\"Classifier pipeline is not loaded.\")\n",
    "        result = self.classifier(input, candidate_labels=CATEGORIES)\n",
    "\n",
    "        return {\n",
    "            \"category\": result[\"labels\"][0],\n",
    "            \"confidence\": round(result[\"scores\"][0], 4)\n",
    "        }\n",
    "```\n",
    "Finally, I deployed the model to baseten\n",
    "```bash\n",
    "cd .\\cat-model\\\n",
    "truss push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196cfb3",
   "metadata": {},
   "source": [
    "While the categorization model was being uploaded, I realized that I could not use the OpenAI SDK for both models. To remedy this, I revised my LLM python code to invoke an API call through requests rather than the OpenAI SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
